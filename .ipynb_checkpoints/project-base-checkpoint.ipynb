{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Bibliotek\n",
    "\n",
    "Ten kod importuje biblioteki niezbędne do kompleksowego modelowania i analizy danych w projekcie wykrywania fraudów:\n",
    "\n",
    "- `pandas`, `numpy`: Umożliwiają efektywną manipulację danymi i obliczenia statystyczne.\n",
    "- `matplotlib`, `seaborn`, `plotly.express`: Zapewniają wizualizację wyników w standardzie publikacji naukowych.\n",
    "- `sklearn`, `imblearn.SMOTE`: Włączają preprocessing (kodowanie, skalowanie, balansowanie klas) i selekcję cech (Mutual Information).\n",
    "- Modele ML: (`XGBoost`, `LightGBM`, `CatBoost`, klasyczne `sklearn`) pozwalają na testowanie różnorodnych metod klasyfikacji.\n",
    "- `tensorflow`, `keras`: Umożliwiają implementację głębokich sieci neuronowych.\n",
    "- `kagglehub`: Automatyzuje pobieranie danych, zwiększając odtwarzalność.\n",
    "\n",
    "Funkcjonalność koncentruje się na przygotowaniu danych i porównaniu podejść ML/DL w zadaniu klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "\n",
    "# Preprocessing and Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Machine Learning models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Data download utilities\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zarządzanie Danymi w Wykrywaniu Fraudów\n",
    "\n",
    "Klasa `DataManager` automatyzuje pobieranie danych z Kaggle (`load_kaggle_dataset`) i ich wczytywanie do `pandas.DataFrame` (`read_data`):\n",
    "\n",
    "## Główne metody:\n",
    "\n",
    "- `__init__`: Inicjalizuje nazwę datasetu i lokalny katalog zapisu.\n",
    "- `load_kaggle_dataset`: Wykorzystuje `kagglehub` do pobrania danych, zapisując je w folderze \"fraud-detection-dataset\" z obsługą błędów.\n",
    "- `read_data`: Wczytuje plik CSV, zwracając `DataFrame` z weryfikacją istnienia pliku.\n",
    "\n",
    "## Cel\n",
    "\n",
    "Funkcjonalność koncentruje się na zapewnieniu odtwarzalnego dostępu do danych, kluczowego w eksperymentach naukowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"Class for managing data loading and storage.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str):\n",
    "        \"\"\"Initialize with dataset name and local directory.\"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.dest_path = os.path.join(self.data_dir, \"fraud-detection-dataset\")\n",
    "    \n",
    "    def load_kaggle_dataset(self) -> str:\n",
    "        \"\"\"Download dataset from Kaggle and save locally.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the saved dataset.\n",
    "        Raises:\n",
    "            Exception: If download fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            path = kagglehub.dataset_download(self.dataset_name)\n",
    "            if os.path.isdir(path) and not os.path.exists(self.dest_path):\n",
    "                shutil.copytree(path, self.dest_path)\n",
    "            elif not os.path.isdir(path):\n",
    "                shutil.copy(path, self.data_dir)\n",
    "            return self.dest_path\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Dataset download failed: {str(e)}\")\n",
    "    \n",
    "    def read_data(self, file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            file_name (str): Name of the CSV file.\n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded DataFrame.\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file does not exist.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.dest_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File {file_path} not found.\")\n",
    "        return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacja\n",
    "\n",
    "Druga komórka inicjalizuje klasę dla konkretnego zbioru danych (`\"ranjitmandal/fraud-detection-dataset-csv\"`) i wczytuje plik `\"Fraud Detection Dataset.csv\"` do zmiennej `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load data\n",
    "data_mgr = DataManager(\"ranjitmandal/fraud-detection-dataset-csv\")\n",
    "dataset_path = data_mgr.load_kaggle_dataset()\n",
    "df = data_mgr.read_data(\"Fraud Detection Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksploracyjna Analiza Danych\n",
    "\n",
    "Klasa `DataExplorer` dostarcza narzędzia do analizy struktury danych i wizualizacji:\n",
    "\n",
    "## Główne metody\n",
    "\n",
    "### `explore_data`\n",
    "- Wyświetla podstawowe informacje o zbiorze danych:\n",
    "  - Wymiary (shape)\n",
    "  - Typy danych\n",
    "  - Liczba unikalnych wartości\n",
    "  - Liczba brakujących wartości\n",
    "\n",
    "### `plot_missing_values`\n",
    "- Generuje wizualizacje brakujących wartości:\n",
    "  - Tworzy siatkę wykresów 3x2\n",
    "  - Zwraca tabelę z rozkładami\n",
    "  - Automatycznie dostosowuje układ do liczby kolumn\n",
    "\n",
    "### `plot_correlations`\n",
    "- Wizualizuje zależności w danych:\n",
    "  - Mapa korelacji między zmiennymi\n",
    "  - Histogram rozkładu klasy docelowej (Fraudulent)\n",
    "\n",
    "### `plot_feature_importance`\n",
    "- Przedstawia rozkład istotnych cech:\n",
    "  - Maksymalnie 4 kolumny w siatce\n",
    "  - Automatyczne skalowanie wykresu\n",
    "  - Porównanie rozkładów względem klasy docelowej\n",
    "\n",
    "## Zastosowanie\n",
    "\n",
    "Narzędzia te umożliwiają kompleksową analizę danych przed modelowaniem, co jest kluczowe dla zrozumienia struktury problemu i potencjalnych wyzwań w detekcji fraudów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class DataExplorer:\n",
    "    \"\"\"Class for exploratory data analysis and visualization using Plotly.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explore_data(df: pd.DataFrame) -> None:\n",
    "        \"\"\"Display basic dataset information.\"\"\"\n",
    "        print(f\"Shape: {df.shape}\\n\")\n",
    "        print(\"Data Types:\\n\", df.dtypes, \"\\n\")\n",
    "        print(\"Unique Values:\\n\", {col: df[col].nunique() for col in df.columns}, \"\\n\")\n",
    "        print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Visualize distribution of missing values in a 3x2 grid and return a data table.\"\"\"\n",
    "        missing_cols = df.columns[df.isnull().any()]\n",
    "        if not missing_cols.size:\n",
    "            print(\"No missing values found.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        n_cols = len(missing_cols)\n",
    "        rows, cols = 3, 2\n",
    "        total_plots = min(n_cols, rows * cols)\n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            subplot_titles=[f\"Distribution of {col}\" for col in missing_cols[:total_plots]],\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        distribution_data = {}\n",
    "        for i, col in enumerate(missing_cols, 1):\n",
    "            if i > total_plots:\n",
    "                break\n",
    "            row = (i - 1) // cols + 1\n",
    "            col_pos = (i - 1) % cols + 1\n",
    "            \n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                hist_data = df[col].dropna()\n",
    "                fig.add_trace(go.Histogram(x=hist_data, name=col, marker_color='red', showlegend=False), row=row, col=col_pos)\n",
    "                counts, bins = np.histogram(hist_data, bins=10)\n",
    "                bin_labels = [f\"{bins[j]:.2f}-{bins[j+1]:.2f}\" for j in range(len(bins)-1)]\n",
    "                distribution_data[col] = pd.Series(counts, index=bin_labels)\n",
    "            else:\n",
    "                counts = df[col].dropna().value_counts()\n",
    "                fig.add_trace(go.Bar(x=counts.index, y=counts.values, name=col, marker_color='red', showlegend=False), row=row, col=col_pos)\n",
    "                distribution_data[col] = counts\n",
    "            \n",
    "            fig.update_xaxes(title_text=col, title_font_size=10, row=row, col=col_pos)\n",
    "            fig.update_yaxes(title_text=\"Count\", title_font_size=10, row=row, col=col_pos)\n",
    "        \n",
    "        fig.update_layout(height=800, width=1200, title_text=\"Distribution of Values in Columns with Missing Data\", bargap=0.2)\n",
    "        fig.show()\n",
    "        \n",
    "        result_df = pd.DataFrame(distribution_data)\n",
    "        result_df.fillna(0, inplace=True)\n",
    "        result_df.to_csv(\"missing_values_distribution.csv\", index_label=\"Category/Bin\")\n",
    "        return result_df\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_correlations(df: pd.DataFrame) -> None:\n",
    "        \"\"\"Visualize correlations and target class distribution using Plotly.\"\"\"\n",
    "        corr_matrix = df.corr().round(4)\n",
    "        fig1 = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            text=corr_matrix.values,\n",
    "            texttemplate=\"%{text:.4f}\",\n",
    "            textfont={\"size\": 10},\n",
    "            colorscale='RdBu',\n",
    "            zmin=-1, zmax=1,\n",
    "            showscale=True\n",
    "        ))\n",
    "        fig1.update_layout(\n",
    "            title=\"Correlation Heatmap\",\n",
    "            width=1200,\n",
    "            height=1200,\n",
    "            xaxis={'tickfont': {'size': 10}},\n",
    "            yaxis={'tickfont': {'size': 10}}\n",
    "        )\n",
    "        fig1.show()\n",
    "        \n",
    "        fig2 = px.histogram(\n",
    "            df, x='Fraudulent',\n",
    "            title=\"Distribution of Transactions\",\n",
    "            width=400, height=300,\n",
    "            color_discrete_sequence=['#1f77b4'],\n",
    "            labels={'Fraudulent': 'Class'}\n",
    "        )\n",
    "        fig2.update_layout(bargap=0.2)\n",
    "        fig2.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_feature_importance(df: pd.DataFrame, important_features: List[str]) -> None:\n",
    "        \"\"\"Visualize important features relative to Fraudulent class in a max 4-column grid.\"\"\"\n",
    "        n_features = len(important_features)\n",
    "        max_cols = 4\n",
    "        n_rows = (n_features + max_cols - 1) // max_cols\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=n_rows,\n",
    "            cols=min(n_features, max_cols),\n",
    "            subplot_titles=[f\"{col}\" for col in important_features],\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, col in enumerate(important_features, 1):\n",
    "            row = (i - 1) // max_cols + 1\n",
    "            col_pos = (i - 1) % max_cols + 1\n",
    "            box_data = [df[df['Fraudulent'] == cls][col].dropna() for cls in df['Fraudulent'].unique()]\n",
    "            fig.add_trace(go.Box(y=box_data[0], name=\"Class 0\", marker_color='#1f77b4', showlegend=False), row=row, col=col_pos)\n",
    "            fig.add_trace(go.Box(y=box_data[1], name=\"Class 1\", marker_color='#ff7f0e', showlegend=False), row=row, col=col_pos)\n",
    "            fig.update_xaxes(title_text=\"Fraudulent\", tickvals=[0, 1], ticktext=[\"0\", \"1\"], title_font_size=12, row=row, col=col_pos)\n",
    "            fig.update_yaxes(title_text=\"Value\", title_font_size=12, row=row, col=col_pos)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=300 * n_rows,\n",
    "            width=300 * max_cols,\n",
    "            title_text=\"Important Features Relative to Fraudulent Class\",\n",
    "            showlegend=False,\n",
    "            title_font_size=20\n",
    "        )\n",
    "        fig.update_annotations(font_size=16)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Danych\n",
    "\n",
    "Klasa `DataPreprocessor` przygotowuje dane do analizy i modelowania:\n",
    "\n",
    "## Główne metody\n",
    "\n",
    "### `preprocess_data`\n",
    "- Imputuje braki (`Transaction_Amount`: mediana)\n",
    "- Usuwa kolumnę `Transaction_ID`\n",
    "\n",
    "### `encode_categorical`\n",
    "- Koduje zmienne kategoryczne:\n",
    "  - `LabelEncoder` dla EDA\n",
    "  - `get_dummies` dla modelowania\n",
    "\n",
    "### `select_features`\n",
    "- Wybiera cechy za pomocą Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from typing import List\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Class for preprocessing data for modeling and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with LabelEncoder.\"\"\"\n",
    "        self.le = LabelEncoder()\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame, target: str = 'Fraudulent') -> pd.DataFrame:\n",
    "        \"\"\"Impute missing values.\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].dtype == 'object':\n",
    "                df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "            else:\n",
    "                if col == 'Transaction_Amount':\n",
    "                    df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "                else:\n",
    "                    df_processed[col] = df_processed[col].fillna(df_processed[col].mean())\n",
    "        return df_processed.drop(['Transaction_ID'], axis=1, errors='ignore')\n",
    "\n",
    "    \n",
    "    def encode_categorical(self, df: pd.DataFrame, columns: List[str], for_eda: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Encode categorical variables.\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        if for_eda:\n",
    "            for col in columns:\n",
    "                df_encoded[col] = self.le.fit_transform(df_encoded[col])\n",
    "        else:\n",
    "            df_encoded = pd.get_dummies(df_encoded, columns=columns, drop_first=True)\n",
    "        return df_encoded\n",
    "    \n",
    "    def select_features(self, X: pd.DataFrame, y: pd.Series, threshold: float = 0) -> List[str]:\n",
    "        \"\"\"Select important features using Mutual Information.\"\"\"\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        important_features = X.columns[mi_scores > threshold].tolist()\n",
    "        print(\"Mutual Information Scores:\", dict(zip(X.columns, mi_scores)))\n",
    "        return important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjalizacja Narzędzi\n",
    "\n",
    "```python\n",
    "# Inicjalizacja\n",
    "data_explorer = DataExplorer()\n",
    "preprocessor = DataPreprocessor()\n",
    "categorical_features = ['Transaction_Type', 'Device_Used', 'Location', 'Payment_Method']\n",
    "```\n",
    "\n",
    "### Komponenty\n",
    "\n",
    "#### Eksploracja Danych\n",
    "- Instancja `data_explorer` klasy `DataExplorer`\n",
    "- Umożliwia wizualizację i analizę struktury danych\n",
    "\n",
    "#### Preprocessing\n",
    "- Instancja `preprocessor` klasy `DataPreprocessor`\n",
    "- Odpowiada za przygotowanie danych do modelowania\n",
    "\n",
    "#### Zmienne Kategoryczne\n",
    "Lista `categorical_features` zawiera kolumny wymagające kodowania:\n",
    "- `Transaction_Type`\n",
    "- `Device_Used`\n",
    "- `Location`\n",
    "- `Payment_Method`\n",
    "\n",
    "### Zastosowanie\n",
    "Ten kod inicjalizuje podstawowe narzędzia potrzebne do analizy i przetwarzania danych w projekcie wykrywania fraudów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects\n",
    "data_explorer = DataExplorer()\n",
    "preprocessor = DataPreprocessor()\n",
    "categorical_features = ['Transaction_Type', 'Device_Used', 'Location', 'Payment_Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza Struktury Danych\n",
    "\n",
    "Wywołuje `explore_data` do wyświetlenia podstawowych informacji o zbiorze danych.\n",
    "\n",
    "### Interpretacja Danych\n",
    "\n",
    "#### Charakterystyka Zbioru\n",
    "\n",
    "- **Rozmiar**: \n",
    "  - 51,000 rekordów\n",
    "  - 12 cech\n",
    "  - Odpowiedni do uczenia maszynowego\n",
    "\n",
    "- **Braki**: \n",
    "  - 4.8-5% w 5 kolumnach\n",
    "  - Wymaga imputacji\n",
    "\n",
    "- **Zmienność**: \n",
    "  - Wysoka w `Transaction_Amount`\n",
    "  - Niska w `Fraudulent` (niezbalansowanie klas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset\n",
    "data_explorer.explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja Braków\n",
    "\n",
    "Generuje wykresy rozkładu wartości w kolumnach z brakami i zapisuje tabelę do CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "data_explorer.plot_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretacja Danych\n",
    "\n",
    "### Rozkład Braków\n",
    "\n",
    "#### Transaction_Amount\n",
    "- Dominacja niskich kwot (47,972 w przedziale 5.03-5,004.31)\n",
    "- 508 transakcji o dużych wartościach\n",
    "- Braki mogą dotyczyć średnich wartości\n",
    "\n",
    "#### Time_of_Transaction\n",
    "- Równomierny rozkład z małymi pikami:\n",
    "  - Nocnymi\n",
    "  - Popołudniowymi\n",
    "- Braki potencjalnie losowe\n",
    "\n",
    "#### Device_Used\n",
    "- Równomierny rozkład między:\n",
    "  - Desktop\n",
    "  - Mobile\n",
    "  - Tablet\n",
    "- 1,530 oznaczonych jako \"Unknown\"\n",
    "- Braki mogą maskować nietypowe urządzenia\n",
    "\n",
    "#### Location\n",
    "- Równomierny rozkład (5,985-6,149)\n",
    "- Braki mogą wskazywać na:\n",
    "  - Losowe występowanie\n",
    "  - Transakcje online\n",
    "\n",
    "#### Payment_Method\n",
    "- Lekka przewaga:\n",
    "  - UPI\n",
    "  - Debit\n",
    "- 1,530 oznaczonych jako \"Invalid\"\n",
    "- Braki mogą ukrywać podejrzane metody płatności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df_cleaned = preprocessor.preprocess_data(df)\n",
    "df_eda = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=True)\n",
    "df_train = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza Statystyczna Braków\n",
    "\n",
    "Klasa `StatisticAnalyse` bada zależność braków od `Fraudulent`:\n",
    "\n",
    "### Metody\n",
    "- `chi_square_test`: Test Chi-kwadrat dla kolumn kategorycznych\n",
    "- `t_test_missing`: Test t dla kolumn numerycznych\n",
    "- `analyze_missing_data`: Wykonuje testy i usuwa tymczasowe kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "class StatisticAnalyse:\n",
    "    \"\"\"Class for statistical analysis of missing data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chi_square_test(df: pd.DataFrame, column: str, target: str = 'Fraudulent') -> tuple:\n",
    "        \"\"\"Perform Chi-square test for categorical column vs target.\"\"\"\n",
    "        df[f'{column}_is_missing'] = df[column].isnull().astype(int)\n",
    "        contingency_table = pd.crosstab(df[f'{column}_is_missing'], df[target])\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "        return chi2, p_value, contingency_table, expected\n",
    "    \n",
    "    @staticmethod\n",
    "    def t_test_missing(df: pd.DataFrame, column: str, target: str = 'Fraudulent') -> tuple:\n",
    "        \"\"\"Perform t-test for numerical column missingness vs target.\"\"\"\n",
    "        df[f'{column}_is_missing'] = df[column].isnull().astype(int)\n",
    "        missing_fraud = df[df[f'{column}_is_missing'] == 1][target]\n",
    "        not_missing_fraud = df[df[f'{column}_is_missing'] == 0][target]\n",
    "        t_stat, p_value = ttest_ind(missing_fraud, not_missing_fraud, equal_var=False)\n",
    "        return t_stat, p_value, missing_fraud.mean(), not_missing_fraud.mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_missing_data(df: pd.DataFrame) -> None:\n",
    "        \"\"\"Analyze missing data and remove temporary columns.\"\"\"\n",
    "        categorical_cols = ['Device_Used', 'Location', 'Payment_Method']\n",
    "        numeric_cols = ['Transaction_Amount', 'Time_of_Transaction']\n",
    "        \n",
    "        print(\"\\n=== Chi-square Tests for Categorical Columns ===\")\n",
    "        for col in categorical_cols:\n",
    "            chi2, p_value, contingency_table, expected = StatisticAnalyse.chi_square_test(df, col)\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(f\"Chi2: {chi2:.4f}, p-value: {p_value:.4f}\")\n",
    "            print(\"Contingency Table:\\n\", contingency_table.to_string())\n",
    "            print(\"Expected Values:\\n\", pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns).to_string())\n",
    "        \n",
    "        print(\"\\n=== T-tests for Numerical Columns ===\")\n",
    "        for col in numeric_cols:\n",
    "            t_stat, p_value, mean_missing, mean_not_missing = StatisticAnalyse.t_test_missing(df, col)\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(f\"T-stat: {t_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "            print(f\"Mean Fraudulent (missing): {mean_missing:.4f}\")\n",
    "            print(f\"Mean Fraudulent (not missing): {mean_not_missing:.4f}\")\n",
    "        \n",
    "        missing_cols = [col for col in df.columns if col.endswith('_is_missing')]\n",
    "        df.drop(columns=missing_cols, inplace=True)\n",
    "        print(\"\\nRemoved temporary columns:\", missing_cols)\n",
    "\n",
    "# Run analysis\n",
    "statistic_analyse = StatisticAnalyse()\n",
    "statistic_analyse.analyze_missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretacja Danych\n",
    "\n",
    "#### Zależność Braków od Klasy\n",
    "\n",
    "- **Device_Used**:\n",
    "  - p=0.0045 < 0.05 – braki zależne (MNAR)\n",
    "  - 6.15% fraudów z brakami vs. 4.86% bez\n",
    "  - Potencjalny sygnał oszustw\n",
    "\n",
    "- **Location**:\n",
    "  - p=0.8618 – braki losowe (MCAR)\n",
    "  - Brak wzorców\n",
    "\n",
    "- **Payment_Method**:\n",
    "  - p=0.1267 – braki losowe (MCAR)\n",
    "  - Brak zależności\n",
    "\n",
    "- **Transaction_Amount**:\n",
    "  - p=0.9268 – braki losowe (MCAR)\n",
    "  - Brak wpływu\n",
    "\n",
    "- **Time_of_Transaction**:\n",
    "  - p=0.4437 – braki losowe (MCAR)\n",
    "  - Brak wpływu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie Danych\n",
    "\n",
    "### Przetwarzanie Danych\n",
    "```python\n",
    "df_cleaned = preprocessor.preprocess_data(df)\n",
    "df_eda = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=True)\n",
    "df_train = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=False)\n",
    "```\n",
    "\n",
    "### Transformacje\n",
    "- **Imputacja braków**:\n",
    "  - Wykonana przez `preprocess_data`\n",
    "  - Zachowuje strukturę danych\n",
    "\n",
    "- **Kodowanie zmiennych**:\n",
    "  - `df_eda`: `LabelEncoder` dla analizy eksploracyjnej\n",
    "  - `df_train`: Kodowanie one-hot dla modelowania\n",
    "\n",
    "### Zmienne Kategoryczne\n",
    "```python\n",
    "categorical_features = [\n",
    "    'Transaction_Type',\n",
    "    'Device_Used',\n",
    "    'Location',\n",
    "    'Payment_Method'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_cleaned = preprocessor.preprocess_data(df)\n",
    "df_eda = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=True)\n",
    "df_train = preprocessor.encode_categorical(df_cleaned, categorical_features, for_eda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza Korelacji\n",
    "\n",
    "### Wizualizacja\n",
    "```python\n",
    "data_explorer.plot_correlations(df_eda)\n",
    "```\n",
    "\n",
    "### Komponenty\n",
    "- **Mapa korelacji**:\n",
    "  - Heatmapa wszystkich zmiennych\n",
    "  - Skala kolorów RdBu (-1 do 1)\n",
    "  - Dokładność do 4 miejsc po przecinku\n",
    "\n",
    "- **Rozkład klasy docelowej**:\n",
    "  - Histogram `Fraudulent`\n",
    "  - Pokazuje niezbalansowanie klas\n",
    "  - Szerokość słupków: 0.2\n",
    "\n",
    "### Interpretacja\n",
    "- Pozwala zidentyfikować:\n",
    "  - Silnie skorelowane zmienne\n",
    "  - Potencjalne problemy z współliniowością\n",
    "  - Zmienne istotne dla detekcji fraudów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "data_explorer.plot_correlations(df_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja i Wizualizacja Cech\n",
    "\n",
    "Wybiera cechy za pomocą Mutual Information i wizualizuje ich rozkład względem `Fraudulent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection and visualization\n",
    "X_eda = df_eda.drop(['Fraudulent'], axis=1)\n",
    "y_eda = df_eda['Fraudulent']\n",
    "important_features = preprocessor.select_features(X_eda, y_eda)\n",
    "data_explorer.plot_feature_importance(df_eda, important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretacja Danych\n",
    "\n",
    "#### Ważność Cech\n",
    "\n",
    "- **Kluczowe predyktory**:\n",
    "  - `Device_Used`: 0.0027 MI\n",
    "  - `Transaction_Type`: 0.0025 MI\n",
    "  - `Location`: 0.0020 MI\n",
    "\n",
    "- **Słabsze predyktory**:\n",
    "  - `Transaction_Amount`: 0.00037 MI\n",
    "  - `Time_of_Transaction`: 0.0014 MI\n",
    "\n",
    "> Cechy z wyższym MI są lepszymi predyktorami fraudów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie Danych i Wizualizacja\n",
    "- `prepare_ml_data`: balansuje dane SMOTE, skaluje i dzieli na zbiory treningowy/testowy.\n",
    "- `get_metrics`: tworzy słownik metryk dla modelu.\n",
    "- `Visualizer`: klasa grupuje metody wizualizacji: macierz pomyłek, wykres słupkowy metryk, interaktywny scatter porównujący modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(df_train: pd.DataFrame, target_col: str = 'Fraudulent', save_dir: str = \"models\") -> tuple:\n",
    "    \"\"\"Prepare data for ML modeling.\"\"\"\n",
    "    X = df_train.drop([target_col], axis=1)\n",
    "    y = df_train[target_col]\n",
    "    X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X, y)\n",
    "    # Initialize and fit scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_resampled)\n",
    "    \n",
    "    # Save scaler\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    scaler_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"Saved scaler to: {scaler_path}\")\n",
    "\n",
    "    # Save training columns\n",
    "    training_cols = list(X.columns)\n",
    "    cols_path = os.path.join(save_dir, \"training_cols.pkl\")\n",
    "    with open(cols_path, \"wb\") as f:\n",
    "        pickle.dump(training_cols, f)\n",
    "    print(f\"Saved training columns to: {cols_path}\")\n",
    "    \n",
    "    return train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "def get_metrics(model_name, y_test, y_pred, y_proba):\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"ROC AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        }\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"Class for visualizing model performance.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, model_name: str) -> None:\n",
    "        \"\"\"Plot confusion matrix using Plotly.\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=[\"Class 0\", \"Class 1\"],\n",
    "            y=[\"Class 0\", \"Class 1\"],\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 12},\n",
    "            colorscale=\"Blues\",\n",
    "            showscale=True,\n",
    "            hoverinfo=\"z\"\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=f\"Confusion Matrix - {model_name}\",\n",
    "            xaxis_title=\"Predicted\",\n",
    "            yaxis_title=\"True\",\n",
    "            width=500,\n",
    "            height=500,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        fig.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_history(history, model_name: str) -> None:\n",
    "        \"\"\"Plot training history (accuracy and loss) using Plotly.\"\"\"\n",
    "        fig = make_subplots(rows=1, cols=2, subplot_titles=(f\"{model_name} - Accuracy\", f\"{model_name} - Loss\"))\n",
    "        \n",
    "        # Accuracy\n",
    "        fig.add_trace(go.Scatter(x=list(range(1, len(history.history['accuracy']) + 1)), y=history.history['accuracy'], mode='lines', name='Train Accuracy', line=dict(color='blue')), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_accuracy']) + 1)), y=history.history['val_accuracy'], mode='lines', name='Validation Accuracy', line=dict(color='orange')), row=1, col=1)\n",
    "        \n",
    "        # Loss\n",
    "        fig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss']) + 1)), y=history.history['loss'], mode='lines', name='Train Loss', line=dict(color='blue')), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss']) + 1)), y=history.history['val_loss'], mode='lines', name='Validation Loss', line=dict(color='orange')), row=1, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            width=1200,\n",
    "            showlegend=True,\n",
    "            title_text=f\"Training History - {model_name}\",\n",
    "            title_font_size=20\n",
    "        )\n",
    "        fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "        fig.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics_bar(results_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Plot bar chart of model performance metrics using Plotly.\"\"\"\n",
    "        metrics = ['Accuracy', 'Recall', 'ROC AUC', 'Precision', 'F1 Score']\n",
    "        melted_df = results_df.melt(id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "        \n",
    "        fig = px.bar(\n",
    "            melted_df,\n",
    "            x='Model',\n",
    "            y='Score',\n",
    "            color='Metric',\n",
    "            barmode='group',\n",
    "            title=\"Model Performance Metrics Comparison\",\n",
    "            color_discrete_sequence=px.colors.qualitative.Vivid,\n",
    "            height=600,\n",
    "            width=1000\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Model\",\n",
    "            yaxis_title=\"Score\",\n",
    "            yaxis_range=[0.80, 1.0],\n",
    "            bargap=0.2,\n",
    "            font=dict(size=12),\n",
    "            legend_title_text=\"Metric\",\n",
    "            legend=dict(title_font_size=12, font_size=10)\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45, showgrid=True, gridcolor='rgba(0,0,0,0.1)')\n",
    "        fig.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)')\n",
    "        fig.show()\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_model_comparison(results_df: pd.DataFrame, min_accuracy: float = 0.95) -> None:\n",
    "        \"\"\"Plot interactive scatter chart for model comparison.\"\"\"\n",
    "        filtered_df = results_df[results_df['Accuracy'] >= min_accuracy]\n",
    "        fig = px.scatter(filtered_df, x=\"Accuracy\", y=\"Recall\", size=\"F1 Score\", color=\"ROC AUC\", hover_data=[\"Model\", \"Precision\"],\n",
    "                         text=\"Model\", title=\"Model Performance Comparison\", color_continuous_scale=px.colors.sequential.Rainbow, size_max=60)\n",
    "        fig.update_traces(textposition='top center')\n",
    "        fig.update_layout(width=1000, height=600, showlegend=True, xaxis_title=\"Accuracy\", yaxis_title=\"Recall (Weighted)\", font=dict(size=12))\n",
    "        fig.show()\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = prepare_ml_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening Modeli ML\n",
    "Klasa `ModelTrainer` trenuje i ocenia 15 modeli ML na przeskalowanych, zbalansowanych danych, zwracając metryki i predykcje:\n",
    "\n",
    "- `__init__`: konfiguruje GPU/CPU i definiuje modele z wagami klas.\n",
    "- `train_and_evaluate`: trenuje modele, oblicza metryki (Accuracy, F1, ROC AUC, Recall, Precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Class for training and evaluating ML models with GPU memory management.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = False, save_dir: str = \"models\"):\n",
    "        \"\"\"Initialize with GPU/CPU configuration and model dictionary.\"\"\"\n",
    "        self.use_gpu = use_gpu\n",
    "        self.device = \"cuda\" if use_gpu and tf.config.list_physical_devices('GPU') else \"cpu\"\n",
    "        self.task_type = \"GPU\" if use_gpu else \"CPU\"\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.models = {\n",
    "            \"Logistic Regression\": LogisticRegression(C=1.0, solver=\"liblinear\", max_iter=500, class_weight=\"balanced\"),\n",
    "            \"Random Forest\": RandomForestClassifier(n_estimators=300, max_depth=12, min_samples_split=5, class_weight=\"balanced_subsample\", random_state=42),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=10, criterion=\"gini\", class_weight=\"balanced\", random_state=42),\n",
    "            \"XGBoost\": XGBClassifier(n_estimators=500, learning_rate=0.03, max_depth=7, scale_pos_weight=5, subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\", use_label_encoder=False, device=self.device),\n",
    "            \"LightGBM\": LGBMClassifier(n_estimators=500, learning_rate=0.03, max_depth=7, num_leaves=60, min_data_in_leaf=5, force_col_wise=True, scale_pos_weight=5, verbose=-1),\n",
    "            \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=500, learning_rate=0.03, max_depth=7, min_samples_split=5),\n",
    "            \"SGD Classifier\": SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=0.0001, max_iter=2000, tol=1e-4, class_weight=\"balanced\"),\n",
    "            \"CatBoost\": CatBoostClassifier(iterations=500, learning_rate=0.03, depth=7, l2_leaf_reg=5, scale_pos_weight=5, verbose=0, task_type=self.task_type),\n",
    "            \"SVM\": SVC(kernel=\"rbf\", C=1.0, probability=True, class_weight=\"balanced\", random_state=42),\n",
    "            \"KNN\": KNeighborsClassifier(n_neighbors=7, weights=\"distance\", n_jobs=-1),\n",
    "            \"AdaBoost\": AdaBoostClassifier(n_estimators=300, learning_rate=0.1, random_state=42),\n",
    "            \"Extra Trees\": ExtraTreesClassifier(n_estimators=300, max_depth=12, min_samples_split=5, class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1),\n",
    "            \"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=500, learning_rate=0.03, max_depth=7, random_state=42),\n",
    "            \"Naive Bayes\": GaussianNB(),\n",
    "            \"MLPClassifier\": MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=500, learning_rate_init=0.001, random_state=42)\n",
    "        }\n",
    "        print(f\"Using {'GPU' if use_gpu else 'CPU'} for supported models.\")\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test) -> tuple:\n",
    "        \"\"\"Train and evaluate models, returning performance metrics and predictions.\"\"\"\n",
    "        results = []\n",
    "        predictions = {}\n",
    "        probabilities = {}\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Starting training: {name}\")\n",
    "            try:\n",
    "                if name == \"XGBoost\":\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                predictions[name] = y_pred\n",
    "                probabilities[name] = y_proba\n",
    "                results.append(get_metrics(name, y_test, y_pred, y_proba))\n",
    "\n",
    "                # Save the trained model\n",
    "                model_path = os.path.join(self.save_dir, f\"{name}_best_model.pkl\")\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                print(f\"Saved model: {model_path}\")\n",
    "                \n",
    "                tf.keras.backend.clear_session()\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {str(e)}\")\n",
    "        results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\n",
    "        return results_df, predictions, probabilities\n",
    "\n",
    "# Train models\n",
    "trainer = ModelTrainer(use_gpu=True)\n",
    "results_df, predictions, probabilities = trainer.train_and_evaluate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlanie Metryk\n",
    "\n",
    "Wyświetla tabelę wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display performance metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Wydajność Modeli ML w Wykrywaniu Fraudów\n",
    "\n",
    "#### W kontekście systemu bankowego do wykrywania fraudów, każda metryka ma specyficzne znaczenie:\n",
    "\n",
    "- Accuracy (dokładność): odsetek poprawnie sklasyfikowanych transakcji. W systemie bankowym wysoka dokładność jest pożądana, ale może być myląca w niezbalansowanych danych (fraudy to ~5% transakcji), gdzie model może ignorować klasę mniejszościową.\n",
    "- Recall (czułość): proporcja wykrytych fraudów (True Positives) do wszystkich rzeczywistych fraudów. Kluczowa w bankowości – wysoki Recall minimalizuje ryzyko przeoczenia oszustw (False Negatives), co ma wysoki koszt finansowy i reputacyjny.\n",
    "- Precision (precyzja): proporcja poprawnie oznaczonych fraudów do wszystkich oznaczonych jako fraud. Wysoka precyzja redukuje fałszywe alarmy (False Positives), co jest istotne, by nie blokować legalnych transakcji klientów.\n",
    "- F1 Score: średnia harmoniczna Precision i Recall. Najważniejsza metryka w tym systemie, bo równoważy wykrywanie fraudów z minimalizacją fałszywych alarmów – optymalizuje koszty operacyjne i zaufanie klientów.\n",
    "- ROC AUC: miara zdolności modelu do rozróżniania klas. Wysokie ROC AUC wskazuje na skuteczność w separacji fraudów od normalnych transakcji, ale nie uwzględnia progu klasyfikacji, co w bankowości wymaga dodatkowego dostrojenia.\n",
    "\n",
    "#### Najlepsze modele\n",
    "\n",
    "##### Gradient Boosting\n",
    "- F1 Score: 0.9708\n",
    "- Accuracy: 0.9708\n",
    "- ROC AUC: 0.9782\n",
    "- Precyzja: 0.9724\n",
    "- Czułość: 0.9708\n",
    "> Najwyższa skuteczność, równowaga precyzji i czułości, doskonałe dopasowanie do danych fraudowych.\n",
    "\n",
    "##### HistGradientBoosting\n",
    "- F1 Score: 0.9644\n",
    "- ROC AUC: 0.9764\n",
    "- Precyzja: 0.9668\n",
    "> Nieco niższa precyzja, ale solidna alternatywa.\n",
    "\n",
    "##### LightGBM\n",
    "- F1 Score: 0.9581\n",
    "- ROC AUC: 0.9776\n",
    "> Stabilne wyniki, choć słabsze od liderów.\n",
    "\n",
    "#### Modele średnie\n",
    "\n",
    "##### MLPClassifier, KNN, XGBoost\n",
    "- F1 Score: 0.9467-0.9474\n",
    "> Dobre, ale tracą na precyzji i czułości wobec topowych modeli gradientowych.\n",
    "\n",
    "##### SVM\n",
    "- F1 Score: 0.9422\n",
    "- Precyzja: 0.9482\n",
    "> Niższa wydajność, ale wysoka precyzja.\n",
    "\n",
    "#### Najsłabsze modele\n",
    "\n",
    "##### Naive Bayes\n",
    "- F1 Score: 0.8026\n",
    "> Najniższa skuteczność, prawdopodobnie z powodu założeń o niezależności cech.\n",
    "\n",
    "##### AdaBoost\n",
    "- F1 Score: 0.8329\n",
    "> Słaba generalizacja w porównaniu do boostingów gradientowych.\n",
    "\n",
    "#### ROC AUC\n",
    "Wartości >0.975 dla Gradient Boosting, XGBoost, LightGBM wskazują na doskonałą zdolność rozróżniania klas, mimo niezbalansowania danych.\n",
    "\n",
    "#### Wnioski\n",
    "1. Gradient Boosting przewyższa inne modele w F1 Score i Accuracy, co sugeruje lepsze uchwycenie nieliniowych wzorców fraudowych.\n",
    "2. Spadek wydajności w Naive Bayes i AdaBoost potwierdza ich ograniczenia w złożonych, niezbalansowanych danych.\n",
    "3. Wysokie ROC AUC we wszystkich modelach (nawet słabszych) wskazuje na dobrą separację klas, ale F1 Score lepiej odzwierciedla praktyczną skuteczność w wykrywaniu fraudów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macierze Pomyłek \n",
    "Wywołuje `plot_confusion_matrix` z klasy `Visualizer` dla każdego modelu, wizualizując błędy klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "visualizer = Visualizer()\n",
    "for model_name, y_pred in predictions.items():\n",
    "    visualizer.plot_confusion_matrix(y_test, y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie modeli\n",
    "- `plot_metrics_bar`: rysuje słupkowy wykres metryk dla wszystkich modeli.\n",
    "- `plot_model_comparison`: tworzy interaktywny scatter dla modeli z Accuracy ≥ 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "Visualizer.plot_metrics_bar(results_df)\n",
    "Visualizer.plot_model_comparison(results_df, min_accuracy=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening Sieci Neuronowej\n",
    "Klasa `NeuralNetworkTrainer` trenuje sieć neuronową na GPU/CPU:\n",
    "\n",
    "- `__init__`: konfiguruje urządzenie i katalogi dla modeli/logs.\n",
    "- `train_and_evaluate`: kompiluje model, trenuje z callbacks (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard), oblicza metryki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTrainer:\n",
    "    \"\"\"Class for training and evaluating neural networks.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = True, save_dir: str = \"models\", log_dir: str = \"logs\"):\n",
    "        \"\"\"Initialize trainer with GPU support and directories.\"\"\"\n",
    "        self.use_gpu = use_gpu\n",
    "        self.save_dir = save_dir\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "        self.device = device_name if use_gpu and device_name == '/device:GPU:0' else '/device:CPU:0'\n",
    "        print(f\"Using {self.device}\")\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test, model, model_name: str = \"Neural Network\",\n",
    "                          epochs: int = 50, batch_size: int = 128, initial_lr: float = 0.0005, threshold: float = 0.5) -> tuple:\n",
    "        \"\"\"Train and evaluate the model, returning results.\"\"\"\n",
    "        with tf.device(self.device):\n",
    "            model.compile(optimizer=Adam(learning_rate=initial_lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            print(f\"\\n{model_name} Model Summary:\")\n",
    "            model.summary()\n",
    "            \n",
    "            callbacks = [\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-8, verbose=1),\n",
    "                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "                ModelCheckpoint(os.path.join(self.save_dir, f\"{model_name}_best_model.keras\"), monitor='val_loss', save_best_only=True, mode='min', verbose=1),\n",
    "                TensorBoard(log_dir=os.path.join(self.log_dir, f\"{model_name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"), histogram_freq=1, write_graph=True, write_images=True, update_freq='epoch')\n",
    "            ]\n",
    "            \n",
    "            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=callbacks, verbose=1)\n",
    "            y_proba = model.predict(X_test)\n",
    "            y_pred = (y_proba > threshold).astype(int).flatten()\n",
    "            results = pd.DataFrame([get_metrics(\"Neural Network\", y_test, y_pred, y_proba)])\n",
    "            return model, history, y_pred, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitorowanie Treningu\n",
    "Włącza TensorBoard w Jupyter/Colab do analizy przebiegu treningu w czasie rzeczywistym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TensorBoard inline (for Jupyter/Colab)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa i Trening DNN\n",
    "- `create_default_model`: tworzy sieć z warstwami gęstymi, Dropout i BatchNormalization.\n",
    "- Trening: inicjalizuje trening, przygotowuje dane i trenuje model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_default_model(input_dim: int) -> Sequential:\n",
    "    \"\"\"Create a default neural network model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trainer = NeuralNetworkTrainer(use_gpu=True)\n",
    "X_train, X_test, y_train, y_test = prepare_ml_data(df_train)\n",
    "default_model = create_default_model(X_train.shape[1])\n",
    "model_name = \"Default_Neural_Network\"\n",
    "model, history, y_pred, dnn_results = trainer.train_and_evaluate(\n",
    "    X_train, X_test, y_train, y_test, default_model, model_name=model_name,\n",
    "    epochs=500, batch_size=64, initial_lr=0.001, threshold=0.425\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykres Historii Treningu\n",
    "Wywołuje `plot_training_history` z klasy `Visualizer` do wizualizacji Accuracy i Loss w Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "Visualizer.plot_training_history(history, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlanie Metryk DNN\n",
    "Wyświetla tabelę wyników sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(dnn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlanie porównanie wszystkich modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision chart\n",
    "Visualizer.plot_model_comparison(pd.concat([results_df, dnn_results], ignore_index=True), 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
